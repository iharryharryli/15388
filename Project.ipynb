{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science on Bitcoin Trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Bitcoin is a new and fast growing currency, and now there are several trading markets of Bitcoin. Compared with stocks, index funds and futures, Bitcoin market approximates a free market more, and is considered less complex to analyze. Thus, this project aims to apply Data Science on Bitcoin Trading, and build an automated Bitcoin trading algorithm that makes money on its own. Most importantly, we want to investigate the potential factors that influence the Bitcoin market, and unveil some characteristics of this market from the perspective of Data Science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content In this Notebook\n",
    "* Getting Raw Data\n",
    "* First Try: Applying Linear Regression\n",
    "* For Trading Decision: Applying Linear Classifaction\n",
    "* For Price Change Prediction: Applying Bayesian Regression for \"latent source model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We found the data from https://bitcoincharts.com/, which provides a huge CSV file around 500MB, that records all the historical transactions in the past two years. We did some preprocessing on the raw data we collect, and wrote the data into a file that makes it easier for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"data/coinbaseUSD.csv\"\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Time'] = df['Time'].apply(lambda x : dtime.datetime.fromtimestamp(float(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.groupby(['Time']).agg({'Price':'mean', 'Quantity':'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"processed.csv\")\n",
    "# convert string to datetime\n",
    "df['Time'] = pd.to_datetime(df['Time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One non-trivial thing we did above is merging transactions that happen almost together, because we don't need the exact time of every individual transaction up to milliseconds precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Try: Applying Linear Regression\n",
    "The first thing we tried is to use the most basic Linear Regression. For each sample, we look at the recent prices change at several fixed data points (10 seconds before, 20s before, 30s before ... to 600s before), and the label is the max price in the next 90 seconds. If an effective model for this data format exists, then we can build our automated trading algorithm on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict max price in the next 90 seconds\n",
    "\n",
    "def extract_feature (current_time, dataframe):\n",
    "    \n",
    "    data_points = [i for i in range(10,600,10)]\n",
    "    #Q_points = [100, 200, 500]\n",
    "    predict_interval = (30, 90)\n",
    "    \n",
    "    lookback = dtime.timedelta(seconds = max(data_points))\n",
    "    \n",
    "    end_i = np.argmin(np.abs(dataframe['Time'] - current_time)) + 1\n",
    "    start_i = np.argmin(np.abs(dataframe['Time'] - (current_time-lookback))) - 1\n",
    "    \n",
    "    predict_interval = (current_time + dtime.timedelta(seconds=predict_interval[0]),\n",
    "                        current_time + dtime.timedelta(seconds=predict_interval[1]))\n",
    "    \n",
    "    predict_slice = dataframe[(dataframe['Time'] >= predict_interval[0]) \n",
    "                                & (dataframe['Time'] <= predict_interval[1])]\n",
    "    \n",
    "    def is_good_sample():\n",
    "        y_threshold = 2 #count\n",
    "        x_threshold = 180  #plus minus seconds\n",
    "        res_count = len(predict_slice)\n",
    "        x_difference = np.abs((dataframe['Time'].iloc[start_i] - \n",
    "                        (current_time-lookback)).total_seconds())\n",
    "        if res_count < y_threshold or x_difference > x_threshold:\n",
    "            return False\n",
    "        return True\n",
    "        \n",
    "    if not is_good_sample():\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    \n",
    "    segment = dataframe.iloc[start_i:end_i]\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    def time_to_float (x):\n",
    "        return (x-current_time).total_seconds()\n",
    "    \n",
    "    result_prices = []\n",
    "    \n",
    "    \n",
    "    x_list = segment['Time'].apply(time_to_float).tolist()\n",
    "    y_list = segment['Price'].tolist()\n",
    "    \n",
    "    current_price = np.interp(0, x_list, y_list)\n",
    "    \n",
    "    for x in data_points:\n",
    "        temp = np.interp(0 - x, x_list, y_list)\n",
    "        result_prices.append(temp - current_price)\n",
    "        \n",
    "    \n",
    "    \n",
    "    result_Q = []\n",
    "    \n",
    "    def consider_Q():\n",
    "        last_time = current_time\n",
    "        for x in Q_points:\n",
    "            new_time = last_time - dtime.timedelta(seconds=x)\n",
    "            temp = dataframe[(dataframe['Time'] > new_time) \n",
    "                             & (dataframe['Time'] <= last_time)]\n",
    "            temp = temp['Quantity'].sum()\n",
    "            result_Q.append(temp)\n",
    "            last_time = new_time\n",
    "    \n",
    "    X = result_prices+result_Q\n",
    "    \n",
    "    Y = np.max(predict_slice['Price']) - current_price\n",
    "    \n",
    "    \n",
    "    \n",
    "    return (X, Y)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.046895424666672625,\n",
       "  0.09379084933334525,\n",
       "  0.12605228709998073,\n",
       "  0.026607842766679823,\n",
       "  -0.07283660156667793,\n",
       "  -0.15935985726667923,\n",
       "  -0.12959241526669985,\n",
       "  -0.09982497326666362,\n",
       "  0.019324528390484375,\n",
       "  0.19806206981905916,\n",
       "  0.3767996112475771,\n",
       "  0.5372050651999984,\n",
       "  0.5326217318666409,\n",
       "  0.5280383985332833,\n",
       "  0.5192050651999693,\n",
       "  0.4721217318666504,\n",
       "  0.4250383985333315,\n",
       "  0.3798717318666718,\n",
       "  0.3519550652000021,\n",
       "  0.3240383985333324,\n",
       "  0.3070383985333365,\n",
       "  0.38828839853334784,\n",
       "  0.46953839853330237,\n",
       "  0.5424967318666631,\n",
       "  0.540830065199998,\n",
       "  0.5391633985333328,\n",
       "  0.5376633985333115,\n",
       "  0.5376633985333115,\n",
       "  0.5376633985333115,\n",
       "  0.5376633985333115,\n",
       "  0.5376633985333115,\n",
       "  0.5376633985333115,\n",
       "  0.5380205413904378,\n",
       "  0.5415919699618712,\n",
       "  0.5451633985333046,\n",
       "  0.5776633985333319,\n",
       "  0.6776633985332978,\n",
       "  0.7776633985333206,\n",
       "  0.35980174233975504,\n",
       "  0.5849264513720414,\n",
       "  0.8100511604042708,\n",
       "  1.0126633985333342,\n",
       "  1.0126633985333342,\n",
       "  1.0126633985333342,\n",
       "  1.0111633985333128,\n",
       "  0.9975547028811889,\n",
       "  0.9964677463594285,\n",
       "  0.9953807898376681,\n",
       "  0.9942938333159077,\n",
       "  0.9932068767941473,\n",
       "  1.000383986768611,\n",
       "  1.0158251632391853,\n",
       "  1.0312663397097594,\n",
       "  1.043305555399968,\n",
       "  1.0247271240666578,\n",
       "  1.0061486927332908,\n",
       "  0.9799526143333424,\n",
       "  0.8851977123333086,\n",
       "  0.7904428103333316],\n",
       " 0.10766339853330464)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test on a particular example\n",
    "extract_feature(dtime.datetime(2016,5,20),df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this helper function that extracts feature for a particular time, we wrote a program that procedurely generates our training set, specified by the time interval and the dataframe we pass as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_samples(start_time, end_time, dataframe):\n",
    "    delta_seconds = 60\n",
    "    \n",
    "    scope_x = start_time - dtime.timedelta(hours=1)\n",
    "    scope_y = end_time + dtime.timedelta(hours=1)\n",
    "    \n",
    "    scope = dataframe[(dataframe['Time'] > scope_x) \n",
    "                             & (dataframe['Time'] < scope_y)].reset_index(drop=True)\n",
    "        \n",
    "    \n",
    "    pointer = start_time\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    while pointer < end_time:\n",
    "        temp = extract_feature(pointer, scope)\n",
    "        if temp is not None:\n",
    "            X.append(temp[0])\n",
    "            Y.append(temp[1])\n",
    "        pointer = pointer + dtime.timedelta(seconds = delta_seconds)\n",
    "    \n",
    "    return (X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a particular training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples = prepare_samples(dtime.datetime(2016,5,20), dtime.datetime(2016,5,21), df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, let's see how well a Linear Regression model can be trained on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg = linear_model.LinearRegression(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.fit(samples[0], samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.080965710556980763"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.score(samples[0], samples[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Trading Decision: Applying Linear Classifaction\n",
    "The score above indicates the coefficient of determination R^2 of the prediction. Such score is a \"terrible\" score, which means the proportion of the variance in the dependent variable that is predictable from the independent variable, so we think the prediction of an exact price change might be too hard. Perhaps training a binary classification SVM that predicts whether there is a chance of making money in the next 90 seconds would be more accurate. This is our immediate next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### For Price Change Prediction: Applying Bayesian Regression for \"latent source model\"\n",
    "As simple Linear Regression performs poorly in predicting Bitcoin price given historical price, we searched for acedemic paper and decide to apply Bayesian Regression for \"latent source model\" proposed by Shah and Zhang.\n",
    "#### Trading Strategy\n",
    "at each time, we either maintain position of +1 Bitcoin, 0 Bitcoin or −1 Bitcoin. At each time instance, we predict the average price movement over the 10 seconds interval, say ∆p, using Bayesian regression discussed by Shah and Zhang in \"Bayesian Regression and Bitcoin\" - if ∆p > t, a threshold, then we buy a bitcoin if current bitcoin position is ≤ 0; if ∆p < −t, then we sell a bitcoin if current position is ≥ 0; else do nothing. The choice of time steps when we make trading decisions as mentioned above are chosen carefully by looking at the recent trends.\n",
    "#### Predicting Price Change\n",
    "Given time- series of price variation of Bitcoin over the interval of few months, measured every 10 second interval, we have a very large time-series (or a vector). We use this historic time series and from it, generate three subsets of time-series data of three different lengths: S1 of time-length 30 minutes, S2 of time-length 60 minutes, and S3 of time-length 120 minutes. Now at a given point of time, to predict the future change ∆p, we use the historical data of three length: previous 30 minutes, 60 minutes and 120 minutes - denoted $x_1$ , $x_2$ and $x_3$ . We use $x_j$ with historical samples $S_j$ for Bayesian regression to predict average price change $∆p_j$ for 1 ≤ j ≤ 3. We also calculate $r = \\frac{v_{bid} − v_{ask}}{v_{bid} + v_{ask}}$ where vbid is total volume people are willing to buy in the top 60 orders and vask is the total volume people are willing to sell in the top 60 orders based on the current order book data. The final estimation ∆p is produced as\n",
    "$$∆p=w_0+ \\sum_{j=1}^{3}􏰆 w_j ∆p^{j}+w_4 r$$\n",
    "where w = (w0, . . . , w4) are learnt parameters. \n",
    "\n",
    "Now on finding Sj,1 ≤ j ≤ 3 and learning w. We divide the entire time duration into three, roughly equal sized, periods. We utilize the first time period to find patterns Sj, 1 ≤ j ≤ 3. The second period is used to learn parameters w and the last third period is used to evaluate the performance of the algorithm. The learning of w is done simply by finding the best linear fit over all choices given the selection of Sj , 1 ≤ j ≤ 3. Now selection of Sj, 1 ≤ j ≤ 3. For this, we take all possible time series of appropriate length (effectively vectors of dimension 180, 360 and 720 respectively for S1,S2 and S3). Each of these form xi and their corresponding label yi is computed by looking at the average price change in the 10 second time interval following the end of time duration of xi. This data repository is extremely large. To facilitate computation on single machine with 128G RAM with 32 cores, we clustered patterns in 100 clusters using k−means algorithm. From these, we chose 20 most effective clusters and took representative patterns from these clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Reference\n",
    "* Shah, Devavrat, and Kang Zhang. ”Bayesian regression and Bitcoin.” arXiv preprint arXiv:1410.1231 (2014)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
